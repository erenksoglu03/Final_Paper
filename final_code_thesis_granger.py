# -*- coding: utf-8 -*-
"""Final_Code_Thesis_Granger.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lPvz62w9wsgfrDUC_omyVAaOzxUExJX9

# IMPORT DATA DEPENDENCIES
"""

#DATA DEPENDECIES
import pandas as pd
import numpy as np
from google.colab import drive
import yfinance as yf
from datetime import datetime, date
import matplotlib.pyplot as plt
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from scipy.special import softmax

"""# UPLOADING DATA

"""

#UPLOADING DATA
df_thesis = pd.read_csv("/content/all_companies_tweets_thesis.csv")

#NUMBER OF TWEETS
print(df_thesis.shape[0])

"""# FILTERING DATA"""

amazon_filter = (df_thesis['amazon'] == True)
condition = (df_thesis['retweet_num'] >= 30) | (df_thesis['like_num'] >= 50)
amazon_reduced = df_thesis.loc[amazon_filter & condition].reset_index()
amazon_reduced.shape

df_thesis.loc[amazon_filter].shape

tesla_filter = (df_thesis['tesla'] == True)
condition = (df_thesis['retweet_num'] >= 100) | (df_thesis['like_num'] >= 200)
tesla_reduced = df_thesis.loc[tesla_filter & condition].reset_index()
tesla_reduced.shape

df_thesis.loc[tesla_filter].shape

apple_filter = (df_thesis['apple'] == True)
condition = (df_thesis['retweet_num'] >= 30) | (df_thesis['like_num'] >= 50)
apple_reduced = df_thesis.loc[apple_filter & condition].reset_index()
apple_reduced.shape

df_thesis.loc[apple_filter].shape

microsoft_filter = (df_thesis['microsoft'] == True)
condition = (df_thesis['retweet_num'] >= 30) | (df_thesis['like_num'] >= 50)
microsoft_reduced = df_thesis.loc[microsoft_filter & condition].reset_index()
microsoft_reduced.shape

df_thesis.loc[microsoft_filter].shape

google_filter = (df_thesis['google'] == True)
condition = (df_thesis['retweet_num'] >= 30) | (df_thesis['like_num'] >= 50)
google_reduced = df_thesis.loc[google_filter & condition].reset_index()
google_reduced.shape

df_thesis.loc[google_filter].shape

#ADJUSTING NETFLIX
netflix = np.zeros(df_thesis.shape[0],dtype=bool)
for i in range(df_thesis.shape[0]):
  if "netflix" in df_thesis.text[i].lower():
    netflix[i] = True

df_thesis["netflix"] = netflix

netflix_filter = (df_thesis['netflix'] == True)
condition = (df_thesis['retweet_num'] >= 5) | (df_thesis['like_num'] >= 5)
netflix_reduced = df_thesis.loc[netflix_filter & condition]
netflix_reduced.shape

df_thesis.loc[netflix_filter].shape

"""# Augumented Dicky Fuller Test (stationarity test)"""

from statsmodels.tsa.stattools import adfuller

def adf_test(timeseries,verbose=False):
    # Perform Dickey-Fuller test
    result = adfuller(timeseries, autolag='AIC')

    # Extract and print results
    print('Results of Dickey-Fuller Test:')
    df_test = pd.Series(result[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
    for key,value in result[4].items():
        df_test['Critical Value (%s)'%key] = value
    if verbose:
      print(df_test)
    if df_test[1] <= 0.05:
      print(f"Series is STATIONARY at 5%, p-value is {df_test[1]}")
    else:
      print(f"Series is NOT STATIONARY at 5%, p-value is {df_test[1]}")

#LOAD PRICE DATA
apple_data_daily = yf.download("AAPL", start="2015-01-01", end="2019-12-31", interval='1d')
apple_data_daily.reset_index(inplace=True)
amazon_data_daily = yf.download("AMZN", start="2015-01-01", end="2019-12-31", interval='1d')
amazon_data_daily.reset_index(inplace=True)
tesla_data_daily = yf.download("TSLA", start="2015-01-01", end="2019-12-31", interval='1d')
tesla_data_daily.reset_index(inplace=True)
microsoft_data_daily = yf.download("MSFT", start="2015-01-01", end="2019-12-31", interval='1d')
microsoft_data_daily.reset_index(inplace=True)
google_data_daily = yf.download("GOOG", start="2015-01-01", end="2019-12-31", interval='1d')
google_data_daily.reset_index(inplace=True)
netflix_data_daily = yf.download("NFLX", start="2015-01-01", end="2019-12-31", interval='1d')
netflix_data_daily.reset_index(inplace=True)

apple_data_daily

# TRANSFORM ALL STOCK MASKS TO BOOLEAN DATATYPE
df_thesis[["apple","amazon","tesla","microsoft","google","netflix", "None"]] = df_thesis[["apple","amazon","tesla","microsoft","google","netflix","None"]].astype(bool)
df_thesis.head()

#COMPUTE TOTAL NUMER OF DAILY TWEETS
def tweet_daily_frequency(df, stock=""):
  label = stock
  stock = df[df[label]==True]
  stock = stock.sort_values(by="post_date", ascending=True)
  stock.reset_index(inplace=True)
  stock = stock.groupby(stock.date).sum()
  stock_grouped = stock[[label]]
  return stock_grouped, label

#COMPUTE DAILY FREQUENCY OF SENTIMENT DIFFERENTIAL BETWEEN POSITIVE AND NEGATIVE LABELED TWEETS
def tweet_daily_frequency_sentiment_diferential(df, stock=""):
  label = stock
  stocks = df.sort_values(by="post_date", ascending=True)
  stocks.reset_index(inplace=True)
  stock = stock[[label]]

  stock_p = stock[(stock[label]==True) & (stock.sentiment=="positive")]
  stock_n = stock[(stock[label]==True) & (stock.sentiment=="negative")]
  stock_pp = stock_p.groupby(stock.date).sum()
  stock_nn = -1*stock_n.groupby(stock.date).sum()
  stock_grouped = stock[[label]]
  return stock_grouped, label

apple_grouped, apple_label = tweet_daily_frequency(df_thesis,"apple")
amazon_grouped, amazon_label = tweet_daily_frequency(df_thesis,"amazon")
tesla_grouped, tesla_label = tweet_daily_frequency(df_thesis,"tesla")
microsoft_grouped, microsoft_label = tweet_daily_frequency(df_thesis,"microsoft")
google_grouped, google_label = tweet_daily_frequency(df_thesis,"google")
netflix_grouped, netflix_label = tweet_daily_frequency(df_thesis,"netflix")

def leftjoin_on_price(prices,sentiments_grouped):
  # Join the two DataFrames on the 'date' column
  price = prices[["Date","Adj Close"]]
  print(price.shape[0])
  sentiments_grouped.index = pd.to_datetime(sentiments_grouped.index)
  sentiments_grouped.reset_index(inplace=True)
  print(sentiments_grouped.shape[0])
  # By default, it's an inner join, meaning it keeps only the common dates
  merged_df = pd.merge(price, sentiments_grouped, left_on='Date', right_on = "date", how='left')
  merged_df.info()
  return merged_df

#NO nan positions for the stock data
apple_merged = leftjoin_on_price(apple_data_daily,apple_grouped)
amazon_merged = leftjoin_on_price(amazon_data_daily,amazon_grouped)
tesla_merged = leftjoin_on_price(tesla_data_daily,tesla_grouped)
microsoft_merged = leftjoin_on_price(microsoft_data_daily,microsoft_grouped)
google_merged = leftjoin_on_price(google_data_daily,google_grouped)
netflix_merged = leftjoin_on_price(netflix_data_daily,netflix_grouped)

import matplotlib.ticker as ticker  # Import the ticker module

def display(data,label):
  # Create a bar plot with consistent data types
  fig, ax = plt.subplots()
  ax.bar(data.index, data[label])  # x-axis and y-axis data match and are valid
  # Labeling and control axis frequency
  ax.set_xlabel("Date")
  ax.set_ylabel("Count")
  ax.set_title(label)
  #ax.xaxis.set_major_locator(ticker.MaxNLocator(3))
  # Display the plot
  plt.show()  # Show the plot

# DISPLAY TOTAL DAILY FREQUENCY
display(apple_grouped, apple_label)
display(amazon_grouped, amazon_label)
display(tesla_grouped, tesla_label)
display(microsoft_grouped, microsoft_label)
display(google_grouped, google_label)
display(netflix_grouped, netflix_label)

from statsmodels.tsa.stattools import grangercausalitytests

def average_of_groups(array, group_size):
  means = []
  for i in range(3,len(array),group_size):
    means.append(np.mean(array[i-3:i]))
  return means

def granger_causality_test(data, max_lag):

    # Perform Granger causality test
    results = grangercausalitytests(data, max_lag, verbose=False)

    # Format results into a dictionary
    results_dict = {}
    for lag, result in results.items():
        lag_results = {}
        for test_name, test_result in result[0].items():
            lag_results[test_name] = test_result
        results_dict[lag] = lag_results

    out = []
    for r1,v1 in results_dict.items():
      for r2, v2 in v1.items():
        out.append(v2[1])

    df = pd.DataFrame()
    df["Lag"] = range(1,1+max_lag)
    df["p_values"] = average_of_groups(out,4)

    return df

"""# SPLINE INTERPOLATION FOR MISSING VALUES




"""

#chose interpolation method: spline interpolation with a polinomial order of 1
def interpolate(dataframe,interpolation_type="spline",order=1,company="netflix"):
  data = dataframe[["Date",company]].copy()
  data.set_index("Date", inplace=True)
  interpolated_mask = np.isnan(np.array(data.values))#~np.isnan(np.array(series.values))
  saved_indexes = list(data.index)
  # Reset the index to numeric values
  data.index = range(1, len(data) + 1)
  # Cubic interpolate missing values
  interpolated_series = data.interpolate(method=interpolation_type, order=order)
  # Set the index back to days of the week
  interpolated_series.index = saved_indexes
  # Convert back to dictionary
  interpolated_data = interpolated_series.to_dict()
  data.values[interpolated_mask] = interpolated_series.values[interpolated_mask]
  data.index = saved_indexes
  return pd.DataFrame(interpolated_data)

interpolated_microsoft = interpolate(microsoft_merged,company="microsoft")
interpolated_apple = interpolate(apple_merged,company="apple")
interpolated_amazon = interpolate(amazon_merged,company="amazon")
interpolated_netflix = interpolate(netflix_merged,company="netflix")
interpolated_tesla = interpolate(tesla_merged,company="tesla")
interpolated_google = interpolate(google_merged,company="google")

#DISPLAY daily total sentiments only on non-empty trading dates; EXAMPLE: NETFLIX
netflix_grouped.head()

#DISPLAY INTERPOLATED OUTPUT; EXAMPLE: NETFLIX
interpolated_netflix.head()

#CONVERT PRICES INTO LOG RETURNS
def calculate_log_returns(prices):
    log_returns = np.log(prices / prices.shift(1))
    return log_returns

"""# STATIONARITY TEST"""

#STATIONARITY TEST
from statsmodels.tsa.stattools import adfuller

# Define a function for Dickey-Fuller test - stationarity test
def adf_test(timeseries,verbose=False):
    # Perform Dickey-Fuller test
    result = adfuller(timeseries, autolag='AIC')

    # Extract and print results
    print('Results of Dickey-Fuller Test:')
    df_test = pd.Series(result[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
    for key,value in result[4].items():
        df_test['Critical Value (%s)'%key] = value
    if verbose:
      print(df_test)
    if df_test[1] <= 0.05:
      print(f"Series is STATIONARY at 5%, p-value is {df_test[1]}")
    else:
      print(f"Series is NOT STATIONARY at 5%, p-value is {df_test[1]}")

"""# GRANGER CAUSALITY"""

from statsmodels.tsa.stattools import grangercausalitytests

def average_of_groups(array, group_size):
  means = []
  for i in range(3,len(array),group_size):
    means.append(np.mean(array[i-3:i]))
  return means

def granger_causality_test(data, max_lag):
    # Perform Granger causality test
    results = grangercausalitytests(data, max_lag, verbose=False)

    # Format results into a dictionary
    results_dict = {}
    for lag, result in results.items():
        lag_results = {}
        for test_name, test_result in result[0].items():
            lag_results[test_name] = test_result
        results_dict[lag] = lag_results

    out = []
    for r1,v1 in results_dict.items():
      for r2, v2 in v1.items():
        out.append(v2[1])

    df = pd.DataFrame()
    df["Lag"] = range(1,max_lag+1)
    df["p_values"] = average_of_groups(out,4)

    return df

def plot_lag_p_values3(lag_p_values, name="",columns=['p_values_S_R', 'p_values_R_S', 'p_values_S_R_RETweight', 'p_values_R_S_RETweight'], text=["Set 1", "Set 2", "Set 3", "Set 4"]):
    # Create a new figure
    fig, ax = plt.subplots(figsize=(7, 5))

    # Extract lag values and p-values for lag_p_values
    lag_values = lag_p_values['Lag']
    p_values1 = lag_p_values[columns[0]]
    p_values2 = lag_p_values[columns[1]]
    p_values3 = lag_p_values[columns[2]]
    p_values4 = lag_p_values[columns[3]]

    # Create scatter plots for different sets
    scatter1 = ax.scatter(lag_values, p_values1, label=text[0], color='skyblue')
    scatter2 = ax.scatter(lag_values, p_values2, label=text[1], color='darkorange')
    scatter3 = ax.scatter(lag_values, p_values3, label=text[2], color='purple')
    scatter4 = ax.scatter(lag_values, p_values4, label=text[3], color='brown')

    # Add horizontal lines at heights 0.05 and 0.1
    ax.axhline(y=0.05, color='lightgreen', linestyle='--', label='0.05 significance')
    ax.axhline(y=0.1, color='green', linestyle='--', label='0.1 significance')

    # Set labels and title
    ax.set_xlabel('Lags')
    ax.set_ylabel('p-values')
    ax.set_title(f'Granger Causality - {name}')


    # Add legend outside the plot
    handles, labels = ax.get_legend_handles_labels()
    ax.legend(handles, labels, loc='center left', bbox_to_anchor=(1, 0.5))  # Legend placed outside plot, to the right

    # Ensure tight layout and show plot
    plt.tight_layout()
    plt.show()

"""# TOTAL SENTIMENT"""

import warnings

# Suppress specific warnings
warnings.filterwarnings("ignore", message="verbose is deprecated since functions should not print results")

"""# APPLE"""

# Data for Apple
date = apple_grouped.index[0]
prices_apple = apple_data_daily["Adj Close"]
volume_apple = apple_data_daily["Volume"]
log_returns_apple = calculate_log_returns(prices_apple)
log_returns_apple.iloc[0]=0

interpolated = interpolated_apple.apple.values  # Sentiments
delta_interpolated = np.log(interpolated[1:] / interpolated[:-1])


df_apple = pd.DataFrame(columns=['Cause', 'Effect'])
df_apple["Cause"] = interpolated # total sentiments
df_apple["Effect"] = log_returns_apple.values#stock returns
granger_apple = granger_causality_test(df_apple,max_lag=15)
granger_apple.rename(columns={'p_values': 'p_values_S_R'}, inplace=True)


df_appleR = pd.DataFrame(columns=['Cause', 'Effect'])
df_appleR["Cause"] = log_returns_apple.values #stock returns
df_appleR["Effect"] = interpolated # total sentiments
granger_appleR = granger_causality_test(df_appleR,15)
granger_appleR.rename(columns={'p_values': 'p_values_R_S'}, inplace=True)


df_appleV = pd.DataFrame(columns=['Cause', 'Effect'])
df_appleV["Cause"] = interpolated #sentiments
df_appleV["Effect"] = volume_apple.values #stock volumes
granger_appleV = granger_causality_test(df_appleV,max_lag=15)
granger_appleV.rename(columns={'p_values': 'p_values_S_V'}, inplace=True)


df_appleVR = pd.DataFrame(columns=['Cause', 'Effect'])
df_appleVR["Cause"] = volume_apple.values #stock volumes
df_appleVR["Effect"] = interpolated #sentiments
granger_appleVR = granger_causality_test(df_appleVR,max_lag=15)
granger_appleVR.rename(columns={'p_values': 'p_values_V_S'}, inplace=True)

granger_output_apple = granger_apple.merge(granger_appleR,on="Lag").merge(granger_appleV,on="Lag").merge(granger_appleVR,on="Lag")
granger_output_apple

#STATIONARITY TEST FOR VOLUME & SENTIMENTS
print(adf_test(interpolated)) #sentiments
print()
print(adf_test(volume_apple)) # volumes

plot_lag_p_values3(granger_output_apple,name="Apple",columns= ['p_values_S_R','p_values_R_S','p_values_S_V','p_values_V_S'],text=["Tot Sentiment--> Returns", "Returns-->Tot Sentiment", "Tot Sentiment-->Volume", "Volume --> Tot Sentiment"])

"""# TESLA"""

# Data for Tesla
date = tesla_grouped.date[0]
prices_tesla = tesla_data_daily["Adj Close"][tesla_data_daily.Date>=date]
volume_tesla = tesla_data_daily["Volume"][tesla_data_daily.Date>=date]
log_returns_tesla = calculate_log_returns(prices_tesla)
log_returns_tesla.iloc[0]=0

df_tesla = pd.DataFrame(columns=['Cause', 'Effect'])
df_teslaR = pd.DataFrame(columns=['Cause', 'Effect'])
df_teslaV = pd.DataFrame(columns=['Cause', 'Effect'])
df_teslaVR = pd.DataFrame(columns=['Cause', 'Effect'])

interpolated = interpolated_tesla[interpolated_tesla.index >= date].tesla.values  # Sentiments
delta_interpolated = np.log(interpolated[1:] / interpolated[:-1])

df_tesla["Cause"] = delta_interpolated # delta sentiment
df_tesla["Effect"] = log_returns_tesla.values[1:]  # Stock returns
granger_tesla = granger_causality_test(df_tesla,15)
granger_tesla.rename(columns={'p_values': 'p_values_S_R'}, inplace=True)


df_teslaR["Cause"] = log_returns_tesla.values[1:]  # Stock returns
df_teslaR["Effect"] = delta_interpolated # delta sentiment
granger_teslaR = granger_causality_test(df_teslaR,15)
granger_teslaR.rename(columns={'p_values': 'p_values_R_S'}, inplace=True)


df_teslaV["Cause"] = delta_interpolated # delta sentiment
df_teslaV["Effect"] = volume_tesla.values[1:]   # Stock volume
granger_teslaV = granger_causality_test(df_teslaV,15)
granger_teslaV.rename(columns={'p_values': 'p_values_S_V'}, inplace=True)


df_teslaVR["Cause"] =  volume_tesla.values[1:] # Stock volume
df_teslaVR["Effect"] = delta_interpolated # delta sentiment
granger_teslaVR = granger_causality_test(df_teslaVR,15)
granger_teslaVR.rename(columns={'p_values': 'p_values_V_S'}, inplace=True)


granger_output_tesla = granger_tesla.merge(granger_teslaR,on="Lag").merge(granger_teslaV,on="Lag").merge(granger_teslaVR,on="Lag")
granger_output_tesla

#STATIONARITY TEST FOR VOLUME & SENTIMENTS
print(adf_test(interpolated)) #sentiments not stationary --> use the change in sentiments instead
print()
print(adf_test(volume_tesla)) # volumes

plot_lag_p_values3(granger_output_tesla,name="Tesla",columns= ['p_values_S_R','p_values_R_S','p_values_S_V','p_values_V_S'],text=["Tot Sentiment--> Returns", "Returns-->Tot Sentiment", "Tot Sentiment-->Volume", "Volume --> Tot Sentiment"])

"""# GOOGLE: stock returns and total sentiment"""

# Data for Google
date = "2018-01-01"
prices_google = google_data_daily["Adj Close"][google_data_daily.Date>=date]
volume_google = google_data_daily["Volume"][google_data_daily.Date>=date]

log_returns_google = calculate_log_returns(prices_google)
log_returns_google.iloc[0]=0

df_google = pd.DataFrame(columns=['Cause', 'Effect'])
df_googleR = pd.DataFrame(columns=['Cause', 'Effect'])
df_googleV = pd.DataFrame(columns=['Cause', 'Effect'])
df_googleRV = pd.DataFrame(columns=['Cause', 'Effect'])

interpolated = interpolated_google[interpolated_google.index >= date].google.values  # Sentiments
delta_sentiment = np.log(interpolated[1:] / interpolated[:-1])

df_google["Cause"] = interpolated # Sentiments
df_google["Effect"] = log_returns_google.values  # Stock returns
granger_google = granger_causality_test(df_google,15)
granger_google.rename(columns={'p_values': 'p_values_S_R'}, inplace=True)


df_googleR["Cause"] = log_returns_google.values  # Stock returns
df_googleR["Effect"] = interpolated # Sentiments
granger_googleR = granger_causality_test(df_googleR,15)
granger_googleR.rename(columns={'p_values': 'p_values_R_S'}, inplace=True)


df_googleV["Cause"] = interpolated  # sentiments
df_googleV["Effect"] = volume_google.values # stock volume
granger_googleV = granger_causality_test(df_googleV,15)
granger_googleV.rename(columns={'p_values': 'p_values_S_V'}, inplace=True)


df_googleRV["Cause"] = volume_google.values  # Stock volume
df_googleRV["Effect"] = interpolated # sentiments
granger_googleRV = granger_causality_test(df_googleRV,15)
granger_googleRV.rename(columns={'p_values': 'p_values_V_S'}, inplace=True)


granger_output_google = granger_google.merge(granger_googleR,on="Lag").merge(granger_googleV,on="Lag").merge(granger_googleRV,on="Lag")
granger_output_google

#STATIONARITY TEST FOR VOLUME & SENTIMENTS
print(adf_test(interpolated)) #sentiments
print()
print(adf_test(volume_tesla)) # volumes

plot_lag_p_values3(granger_output_google,name="Google",columns= ['p_values_S_R','p_values_R_S','p_values_S_V','p_values_V_S'],text=["Tot Sentiment--> Returns", "Returns-->Tot Sentiment", "Tot Sentiment-->Volume", "Volume --> Tot Sentiment"])

"""# AMAZON and total sentiment"""

# Data for Amazon
date = "2017-06-01"
prices_amazon = amazon_data_daily["Adj Close"][amazon_data_daily.Date>=date]
volume_amazon = amazon_data_daily["Volume"][amazon_data_daily.Date>=date]
log_returns_amazon = calculate_log_returns(prices_amazon)
log_returns_amazon.iloc[0]=0

df_amazon = pd.DataFrame(columns=['Cause', 'Effect'])
df_amazonR = pd.DataFrame(columns=['Cause', 'Effect'])
df_amazonV = pd.DataFrame(columns=['Cause', 'Effect'])
df_amazonRV = pd.DataFrame(columns=['Cause', 'Effect'])

interpolated = interpolated_amazon[interpolated_amazon.index >= date].amazon.values  # Sentiments
delta_sentiment = np.log(interpolated[1:] / interpolated[:-1])

df_amazon["Cause"] = interpolated  # Sentiments
df_amazon["Effect"] = log_returns_amazon.values  # Stock returns
granger_amazon = granger_causality_test(df_amazon, 15)
granger_amazon.rename(columns={'p_values': 'p_values_S_R'}, inplace=True)


df_amazonR["Cause"] = log_returns_amazon.values  # Stock returns
df_amazonR["Effect"] = interpolated  # Sentiments
granger_amazonR = granger_causality_test(df_amazonR, 15)
granger_amazonR.rename(columns={'p_values': 'p_values_R_S'}, inplace=True)


df_amazonV["Cause"] = interpolated  # Sentiments
df_amazonV["Effect"] = volume_amazon.values  # Stock volume
granger_amazonV = granger_causality_test(df_amazonV, 15)
granger_amazonV.rename(columns={'p_values': 'p_values_S_V'}, inplace=True)

df_amazonRV["Cause"] = volume_amazon.values  # Stock volume
df_amazonRV["Effect"] = interpolated  # Sentiments
granger_amazonRV = granger_causality_test(df_amazonRV, 15)
granger_amazonRV.rename(columns={'p_values': 'p_values_V_S'}, inplace=True)

granger_output_amazon = granger_amazon.merge(granger_amazonR,on="Lag").merge(granger_amazonV,on="Lag").merge(granger_amazonRV,on="Lag")
granger_output_amazon

#STATIONARITY TEST FOR VOLUME & SENTIMENTS
print(adf_test(interpolated)) #sentiments
print()
print(adf_test(volume_amazon)) # volumes

plot_lag_p_values3(granger_output_amazon, name="Amazon",columns= ['p_values_S_R','p_values_R_S','p_values_S_V','p_values_V_S'],text=["Tot Sentiment--> Returns", "Returns-->Tot Sentiment", "Tot Sentiment-->Volume", "Volume --> Tot Sentiment"])

"""# NETFLIX"""

# Data for Netflix
date = "2018-01-01"
prices_netflix = netflix_data_daily["Adj Close"][netflix_data_daily.Date >= date]
volume_netflix = netflix_data_daily["Volume"][netflix_data_daily.Date >= date]
log_returns_netflix = calculate_log_returns(prices_netflix)
log_returns_netflix.iloc[0] = 0

# Create DataFrames
df_netflix = pd.DataFrame(columns=['Cause', 'Effect'])
df_netflixR = pd.DataFrame(columns=['Cause', 'Effect'])
df_netflixV = pd.DataFrame(columns=['Cause', 'Effect'])
df_netflixRV = pd.DataFrame(columns=['Cause', 'Effect'])

# Interpolated data and sentiment
interpolated = interpolated_netflix[interpolated_netflix.index >= date].netflix.values  # Sentiments
delta_sentiment = np.log(interpolated[1:] / interpolated[:-1])

# Netflix data analysis
df_netflix["Cause"] = delta_sentiment  # Sentiments
df_netflix["Effect"] = log_returns_netflix.values[1:]  # Stock returns
granger_netflix = granger_causality_test(df_netflix, 15)
granger_netflix.rename(columns={'p_values': 'p_values_S_R'}, inplace=True)

# Stock returns causing sentiment
df_netflixR["Cause"] = log_returns_netflix.values[1:]  # Stock returns
df_netflixR["Effect"] = delta_sentiment  # Sentiments
granger_netflixR = granger_causality_test(df_netflixR, 15)
granger_netflixR.rename(columns={'p_values': 'p_values_R_S'}, inplace=True)

# Sentiments causing stock volume
df_netflixV["Cause"] = interpolated  # Sentiments
df_netflixV["Effect"] = volume_netflix.values  # Stock volume
granger_netflixV = granger_causality_test(df_netflixV, 15)
granger_netflixV.rename(columns={'p_values': 'p_values_S_V'}, inplace=True)

# Stock volume causing sentiment
df_netflixRV["Cause"] = volume_netflix.values  # Stock volume
df_netflixRV["Effect"] = interpolated  # Sentiments
granger_netflixRV = granger_causality_test(df_netflixRV, 15)
granger_netflixRV.rename(columns={'p_values': 'p_values_V_S'}, inplace=True)

# Merging DataFrames
granger_output_netflix = granger_netflix.merge(granger_netflixR, on="Lag").merge(granger_netflixV, on="Lag").merge(granger_netflixRV, on="Lag")
granger_output_netflix

#STATIONARITY TEST FOR VOLUME & SENTIMENTS
print(adf_test(interpolated)) #sentiments
print()
print(adf_test(volume_netflix)) # volumes

plot_lag_p_values3(granger_output_netflix,name="Netflix",columns= ['p_values_S_R','p_values_R_S','p_values_S_V','p_values_V_S'],text=["Tot Sentiment--> Returns", "Returns-->Tot Sentiment", "Tot Sentiment-->Volume", "Volume --> Tot Sentiment"])

"""# MICROSOFT"""

# Data for Microsoft
date = "2017-06-01"
prices_microsoft = microsoft_data_daily["Adj Close"][microsoft_data_daily.Date >= date]
volume_microsoft = microsoft_data_daily["Volume"][microsoft_data_daily.Date >= date]
log_returns_microsoft = calculate_log_returns(prices_microsoft)
log_returns_microsoft.iloc[0] = 0

# Create DataFrames for Microsoft
df_microsoft = pd.DataFrame(columns=['Cause', 'Effect'])
df_microsoftR = pd.DataFrame(columns=['Cause', 'Effect'])
df_microsoftV = pd.DataFrame(columns=['Cause', 'Effect'])
df_microsoftRV = pd.DataFrame(columns=['Cause', 'Effect'])

# Interpolated data and sentiment for Microsoft
interpolated = interpolated_microsoft[interpolated_microsoft.index >= date].microsoft.values  # Sentiments
delta_sentiment = np.log(interpolated[1:] / interpolated[:-1])

# Microsoft data analysis
df_microsoft["Cause"] = interpolated  # Sentiments
df_microsoft["Effect"] = log_returns_microsoft.values  # Stock returns
granger_microsoft = granger_causality_test(df_microsoft, 15)
granger_microsoft.rename(columns={'p_values': 'p_values_S_R'}, inplace=True)

# Stock returns causing sentiment for Microsoft
df_microsoftR["Cause"] = log_returns_microsoft.values  # Stock returns
df_microsoftR["Effect"] = interpolated  # Sentiments
granger_microsoftR = granger_causality_test(df_microsoftR, 15)
granger_microsoftR.rename(columns={'p_values': 'p_values_R_S'}, inplace=True)

# Sentiments causing stock volume for Microsoft
df_microsoftV["Cause"] = interpolated  # Sentiments
df_microsoftV["Effect"] = volume_microsoft.values  # Stock volume
granger_microsoftV = granger_causality_test(df_microsoftV, 15)
granger_microsoftV.rename(columns={'p_values': 'p_values_S_V'}, inplace=True)

# Stock volume causing sentiment for Microsoft
df_microsoftRV["Cause"] = volume_microsoft.values  # Stock volume
df_microsoftRV["Effect"] = interpolated  # Sentiments
granger_microsoftRV = granger_causality_test(df_microsoftRV, 15)
granger_microsoftRV.rename(columns={'p_values': 'p_values_V_S'}, inplace=True)

# Merging DataFrames on "Lag"
granger_output_microsoft = (
    granger_microsoft
    .merge(granger_microsoftR, on="Lag")
    .merge(granger_microsoftV, on="Lag")
    .merge(granger_microsoftRV, on="Lag")
)

granger_output_microsoft

#STATIONARITY TEST FOR VOLUME & SENTIMENTS
print(adf_test(interpolated)) #sentiments
print()
print(adf_test(volume_microsoft)) # volumes

plot_lag_p_values3(granger_output_microsoft,name="Microsoft",columns= ['p_values_S_R','p_values_R_S','p_values_S_V','p_values_V_S'],text=["Tot Sentiment--> Returns", "Returns-->Tot Sentiment", "Tot Sentiment-->Volume", "Volume --> Tot Sentiment"])

"""# DATA WITH SENTIMENT DIFFERENTIAL

---


"""

#DOWNLOAD FROM DRIVE THE CSVs
import gdown
gdown.download(id="13lFcEWhA9IP8esQZnEZQGMa_JqSQzFqa",output="netflix_reduced_sentiment.csv")
gdown.download(id="1-rnXZNPkrMZgJhMy42aXiJ4lGOJxIiOM",output="microsoft_reduced_sentiment.csv")
gdown.download(id="1x8rva0cVRGnRXQlUOtcPfFyfRG-waaYZ",output="tesla_reduced_sentiment.csv")
gdown.download(id="1nTzlBGuKr1L6DRvbuulQcNSYf_tgObUH",output="apple_reduced_sentiment.csv")
gdown.download(id="1mZd_Kd7qBkdGw7Kuo1hoD-I-HErWJPDG",output="amazon_reduced_sentiment.csv")
gdown.download(id="1TLIdL_BK8PFqbWSb5yC57ksXcpjUQa-Y",output="google_reduced_sentiment.csv")

#save them as dataframes
netflix_with_sentiment = pd.read_csv("netflix_reduced_sentiment.csv")
microsoft_with_sentiment = pd.read_csv("microsoft_reduced_sentiment.csv")
tesla_with_sentiment = pd.read_csv("tesla_reduced_sentiment.csv")
apple_with_sentiment = pd.read_csv("apple_reduced_sentiment.csv")
amazon_with_sentiment = pd.read_csv("amazon_reduced_sentiment.csv")
google_with_sentiment = pd.read_csv("google_reduced_sentiment.csv")

#DATAFRAME WITH LABELED SENTIMENTS; EXAMPLE: AMAZON
amazon_with_sentiment

def sentiment_difference(df):
  # Group by date and sentiment, then count the occurrences
  daily_sentiment_counts = df.groupby(['date', 'sentiment']).size().reset_index()  # Corrected method
  daily_sentiments_pivot = daily_sentiment_counts.pivot(index='date', columns='sentiment', values=0)
  # Fill missing values with 0 (if there are dates without one of the sentiment types)
  daily_sentiments_pivot = daily_sentiments_pivot.fillna(0)

  # Calculate the difference between positive and negative sentiment counts
  daily_sentiment_difference = daily_sentiments_pivot['positive'] - daily_sentiments_pivot['negative']

  # Create a new DataFrame to hold the result
  sentiment_difference = pd.DataFrame({'date': daily_sentiment_difference.index,'sentiment_difference': daily_sentiment_difference})
  sentiment_difference.drop("date", axis=1, inplace=True)


  return sentiment_difference

def sentiment_and_retweet_weighted(df):
    # Group by date and sentiment, then count the occurrences
    daily_sentiment_counts = df.groupby(['date', 'sentiment']).size().reset_index(name='count')
    daily_retweet_counts = df.groupby(['date', 'sentiment'])['retweet_num'].sum().reset_index(name='retweet_sum')

    # Create pivot tables for sentiment counts and retweet sums
    daily_sentiments_pivot = daily_sentiment_counts.pivot(index='date', columns='sentiment', values='count')
    daily_retweets_pivot = daily_retweet_counts.pivot(index='date', columns='sentiment', values='retweet_sum')

    # Fill missing values with 0
    daily_sentiments_pivot.fillna(0, inplace=True)
    daily_retweets_pivot.fillna(0, inplace=True)

    # Multiply sentiment counts by the corresponding retweet sums
    weighted_positive = daily_sentiments_pivot['positive'] * daily_retweets_pivot['positive']
    weighted_negative = daily_sentiments_pivot['negative'] * daily_retweets_pivot['negative']

    # Calculate the difference between weighted positive and negative sentiments
    daily_sentiment_difference = weighted_positive - weighted_negative

    # Create a DataFrame to hold the results
    weighted_sentiment_difference = pd.DataFrame({
        'date': daily_sentiment_difference.index,
        'weighted_sentiment_difference': daily_sentiment_difference
    })
    weighted_sentiment_difference.drop("date", axis=1, inplace=True)

    return weighted_sentiment_difference

#DAILY SENTIMENT DIFF
sentiment_difference_amazon = sentiment_difference(amazon_with_sentiment)
sentiment_difference_google = sentiment_difference(google_with_sentiment)
sentiment_difference_netflix = sentiment_difference(netflix_with_sentiment)
sentiment_difference_microsoft = sentiment_difference(microsoft_with_sentiment)
sentiment_difference_apple = sentiment_difference(apple_with_sentiment)
sentiment_difference_tesla = sentiment_difference(tesla_with_sentiment)

#DAILY SENTIMENT DIFF WEIGHTED BY CORRESPONDING RETWEETS
weighted_sentiment_difference_amazon = sentiment_and_retweet_weighted(amazon_with_sentiment)
weighted_sentiment_difference_google = sentiment_and_retweet_weighted(google_with_sentiment)
weighted_sentiment_difference_netflix = sentiment_and_retweet_weighted(netflix_with_sentiment)
weighted_sentiment_difference_microsoft = sentiment_and_retweet_weighted(microsoft_with_sentiment)
weighted_sentiment_difference_apple = sentiment_and_retweet_weighted(apple_with_sentiment)
weighted_sentiment_difference_tesla = sentiment_and_retweet_weighted(tesla_with_sentiment)

#LOAD PRICE DATA
apple_data_daily = yf.download("AAPL", start="2015-01-01", end="2019-12-31", interval='1d')
apple_data_daily.reset_index(inplace=True)

amazon_data_daily = yf.download("AMZN", start="2015-01-01", end="2019-12-31", interval='1d')
amazon_data_daily.reset_index(inplace=True)

tesla_data_daily = yf.download("TSLA", start="2015-01-01", end="2019-12-31", interval='1d')
tesla_data_daily.reset_index(inplace=True)

microsoft_data_daily = yf.download("MSFT", start="2015-01-01", end="2019-12-31", interval='1d')
microsoft_data_daily.reset_index(inplace=True)

google_data_daily = yf.download("GOOG", start="2015-01-01", end="2019-12-31", interval='1d')
google_data_daily.reset_index(inplace=True)

netflix_data_daily = yf.download("NFLX", start="2015-01-01", end="2019-12-31", interval='1d')
netflix_data_daily.reset_index(inplace=True)

facebook_data_daily = yf.download("META", start="2015-01-01", end="2019-12-31", interval='1d')
facebook_data_daily.reset_index(inplace=True)

def merge_sentiment_stockprice(sentiment,stockprices):
  stockprices.set_index("Date", inplace=True)
  sentiment.index = pd.to_datetime(sentiment.index)
  sentiment_difference_merged = pd.merge(stockprices[["Adj Close","Volume"]],sentiment[["sentiment_difference"]], left_index=True, right_index=True, how="left")
  return sentiment_difference_merged

def merge_weighted_sentiment_stockprice(sentiment,stockprices):
  sentiment.index = pd.to_datetime(sentiment.index)
  sentiment_difference_merged = pd.merge(stockprices[["Adj Close","Volume"]],sentiment[["weighted_sentiment_difference"]], left_index=True, right_index=True, how="left")
  return sentiment_difference_merged

merged_sentiment_stockprice_amazon = merge_sentiment_stockprice(sentiment_difference_amazon,amazon_data_daily)
merged_weighted_sentiment_stockprice_amazon = merge_weighted_sentiment_stockprice(weighted_sentiment_difference_amazon,amazon_data_daily)

merged_sentiment_stockprice_apple = merge_sentiment_stockprice(sentiment_difference_apple,apple_data_daily)
merged_weighted_sentiment_stockprice_apple = merge_weighted_sentiment_stockprice(weighted_sentiment_difference_apple,apple_data_daily)

merged_sentiment_stockprice_google = merge_sentiment_stockprice(sentiment_difference_google,google_data_daily)
merged_weighted_sentiment_stockprice_google = merge_weighted_sentiment_stockprice(weighted_sentiment_difference_google,google_data_daily)

merged_sentiment_stockprice_netflix = merge_sentiment_stockprice(sentiment_difference_netflix,netflix_data_daily)
merged_weighted_sentiment_stockprice_netflix = merge_weighted_sentiment_stockprice(weighted_sentiment_difference_netflix,netflix_data_daily)

merged_sentiment_stockprice_microsoft = merge_sentiment_stockprice(sentiment_difference_microsoft,microsoft_data_daily)
merged_weighted_sentiment_stockprice_microsoft = merge_weighted_sentiment_stockprice(weighted_sentiment_difference_microsoft,microsoft_data_daily)

merged_sentiment_stockprice_tesla = merge_sentiment_stockprice(sentiment_difference_tesla,tesla_data_daily)
merged_weighted_sentiment_stockprice_tesla = merge_weighted_sentiment_stockprice(weighted_sentiment_difference_tesla,tesla_data_daily)

#chose interpolation method: spline interpolation with a polinomial order of 1
def interpolate2(dataframe,interpolation_type="spline",order=1):
  data = dataframe.copy()
  #data.set_index("Date", inplace=True)
  interpolated_mask = np.isnan(np.array(data.sentiment_difference.values))
  saved_indexes = list(data.index)
  # Reset the index to numeric values
  data.index = range(1, len(data) + 1)
  interpolated_series = data.sentiment_difference.interpolate(method=interpolation_type, order=order)
  # Set the index back to days of the week
  interpolated_series.index = saved_indexes
  # Convert back to dictionary
  interpolated_data = interpolated_series.to_dict()
  data.sentiment_difference.values[interpolated_mask] = interpolated_series.values[interpolated_mask]
  data.index = saved_indexes
  return data

#chose interpolation method: spline interpolation with a polinomial order of 1
def interpolate3(dataframe,interpolation_type="spline",order=1):
  data = dataframe.copy()
  interpolated_mask = np.isnan(np.array(data.weighted_sentiment_difference.values))
  saved_indexes = list(data.index)
  # Reset the index to numeric values
  data.index = range(1, len(data) + 1)
  interpolated_series = data.weighted_sentiment_difference.interpolate(method=interpolation_type, order=order)
  # Set the index back to days of the week
  interpolated_series.index = saved_indexes
  # Convert back to dictionary
  interpolated_data = interpolated_series.to_dict()
  data.weighted_sentiment_difference.values[interpolated_mask] = interpolated_series.values[interpolated_mask]
  data.index = saved_indexes
  return data

"""# APPLE SENTIMENT & WEIGHTED SENTIMENT DIFFERENTIAL"""

#APPLE
apple_interpolated = interpolate2(merged_sentiment_stockprice_apple)
apple_interpolated_2016_ = apple_interpolated[apple_interpolated.index>="2016-01-01"].sentiment_difference.values

apple_interpolatedW = interpolate3(merged_weighted_sentiment_stockprice_apple)
apple_interpolated_2016_W = apple_interpolatedW[apple_interpolatedW.index>="2016-01-01"].weighted_sentiment_difference.values

apple_prices_2016_ = apple_data_daily[apple_data_daily.index>="2016-01-01"][["Adj Close"]].values


df_apple_sentiment = pd.DataFrame(columns=['Cause', 'Effect'])
df_apple_sentiment["Cause"] = apple_interpolated_2016_[1:]
df_apple_sentiment["Effect"] = np.log(apple_prices_2016_[1:]/apple_prices_2016_[:-1])
granger_apple_sentiment = granger_causality_test(df_apple_sentiment,max_lag=15)
granger_apple_sentiment.rename(columns={'p_values': 'p_values_S_R'}, inplace=True)

df_apple_sentimentR = pd.DataFrame(columns=['Cause', 'Effect'])
df_apple_sentimentR["Cause"] = list(np.log(apple_prices_2016_[1:]/apple_prices_2016_[:-1]))
df_apple_sentimentR["Effect"] = apple_interpolated_2016_[1:]
granger_apple_sentimentR = granger_causality_test(df_apple_sentimentR,max_lag=15)
granger_apple_sentimentR.rename(columns={'p_values': 'p_values_R_S'}, inplace=True)

#############sentiment######differential###############
df_apple_sentimentW = pd.DataFrame(columns=['Cause', 'Effect'])
df_apple_sentimentW["Cause"] = apple_interpolated_2016_W[1:]
df_apple_sentimentW["Effect"] = np.log(apple_prices_2016_[1:]/apple_prices_2016_[:-1])
granger_apple_sentimentW = granger_causality_test(df_apple_sentimentW,max_lag=15)
granger_apple_sentimentW.rename(columns={'p_values': 'p_values_S_R_RETweight'}, inplace=True)

df_apple_sentimentRW = pd.DataFrame(columns=['Cause', 'Effect'])
df_apple_sentimentRW["Cause"] = list(np.log(apple_prices_2016_[1:]/apple_prices_2016_[:-1]))
df_apple_sentimentRW["Effect"] = apple_interpolated_2016_W[1:]
granger_apple_sentimentRW = granger_causality_test(df_apple_sentimentRW,max_lag=15)
granger_apple_sentimentRW.rename(columns={'p_values': 'p_values_R_S_RETweight'}, inplace=True)

granger_output_apple = granger_apple_sentiment.merge(granger_apple_sentimentR,on="Lag").merge(granger_apple_sentimentW,on="Lag").merge(granger_apple_sentimentRW,on="Lag")
granger_output_apple

plot_lag_p_values3(granger_output_apple,name="Apple",columns= ['p_values_S_R','p_values_R_S','p_values_S_R_RETweight','p_values_R_S_RETweight'],text=["Sentiment Diff-->Returns", "Returns-->Sentiment Diff", "Weighted Sentiment Diff-->Returns", "Returns-->Weighted Sentiment Diff"])

"""# GOOGLE SENTIMENT & WEIGHTED SENTIMENT DIFFERENTIAL"""

#GOOGLE
# Set up the data for Google
google_interpolated = interpolate2(merged_sentiment_stockprice_google)
google_interpolated_2016_ = google_interpolated[google_interpolated.index >= "2016-01-01"].sentiment_difference.values

google_interpolatedW = interpolate3(merged_weighted_sentiment_stockprice_google)
google_interpolated_2016_W = google_interpolatedW[google_interpolatedW.index >= "2016-01-01"].weighted_sentiment_difference.values

google_prices_2016_ = google_data_daily[google_data_daily.index >= "2016-01-01"][["Adj Close"]].values

# Create DataFrames for sentiment and weighted sentiment
df_google_sentiment = pd.DataFrame(columns=['Cause', 'Effect'])
df_google_sentiment["Cause"] = google_interpolated_2016_[1:]
df_google_sentiment["Effect"] = np.log(google_prices_2016_[1:]/google_prices_2016_[:-1])
granger_google_sentiment = granger_causality_test(df_google_sentiment, max_lag=15)
granger_google_sentiment.rename(columns={'p_values': 'p_values_S_R'}, inplace=True)

df_google_sentimentR = pd.DataFrame(columns=['Cause', 'Effect'])
df_google_sentimentR["Cause"] = list(np.log(google_prices_2016_[1:]/google_prices_2016_[:-1]))
df_google_sentimentR["Effect"] = google_interpolated_2016_[1:]
granger_google_sentimentR = granger_causality_test(df_google_sentimentR, max_lag=15)
granger_google_sentimentR.rename(columns={'p_values': 'p_values_R_S'}, inplace=True)

# Differential sentiment and retweet-weighted sentiment
df_google_sentimentW = pd.DataFrame(columns=['Cause', 'Effect'])
df_google_sentimentW["Cause"] = google_interpolated_2016_W[1:]
df_google_sentimentW["Effect"] = np.log(google_prices_2016_[1:]/google_prices_2016_[:-1])
granger_google_sentimentW = granger_causality_test(df_google_sentimentW, max_lag=15)
granger_google_sentimentW.rename(columns={'p_values': 'p_values_S_R_RETweight'}, inplace=True)

df_google_sentimentRW = pd.DataFrame(columns=['Cause', 'Effect'])
df_google_sentimentRW["Cause"] = list(np.log(google_prices_2016_[1:]/google_prices_2016_[:-1]))
df_google_sentimentRW["Effect"] = google_interpolated_2016_W[1:]
granger_google_sentimentRW = granger_causality_test(df_google_sentimentRW, max_lag=15)
granger_google_sentimentRW.rename(columns={'p_values': 'p_values_R_S_RETweight'}, inplace=True)

# Combine all results into a single DataFrame
granger_output_google = granger_google_sentiment.merge(granger_google_sentimentR, on="Lag") \
                                                .merge(granger_google_sentimentW, on="Lag") \
                                                .merge(granger_google_sentimentRW, on="Lag")

granger_output_google

plot_lag_p_values3(granger_output_google,name="Google",columns= ['p_values_S_R','p_values_R_S','p_values_S_R_RETweight','p_values_R_S_RETweight'],text=["Sentiment Diff-->Returns", "Returns-->Sentiment Diff", "Weighted Sentiment Diff-->Returns", "Returns-->Weighted Sentiment Diff"])

"""# AMAZON SENTIMENT & WEIGHTED SENTIMENT DIFFERENTIAL"""

#AMAZON
# Set up the data for Amazon
amazon_interpolated = interpolate2(merged_sentiment_stockprice_amazon)
amazon_interpolated_2016_ = amazon_interpolated[amazon_interpolated.index >= "2016-01-01"].sentiment_difference.values

amazon_interpolatedW = interpolate3(merged_weighted_sentiment_stockprice_amazon)
amazon_interpolated_2016_W = amazon_interpolatedW[amazon_interpolatedW.index >= "2016-01-01"].weighted_sentiment_difference.values

amazon_prices_2016_ = amazon_data_daily[amazon_data_daily.index >= "2016-01-01"][["Adj Close"]].values

# Create DataFrames for sentiment and weighted sentiment
df_amazon_sentiment = pd.DataFrame(columns=['Cause', 'Effect'])
df_amazon_sentiment["Cause"] = amazon_interpolated_2016_[1:]
df_amazon_sentiment["Effect"] = np.log(amazon_prices_2016_[1:]/amazon_prices_2016_[:-1])
granger_amazon_sentiment = granger_causality_test(df_amazon_sentiment, max_lag=15)
granger_amazon_sentiment.rename(columns={'p_values': 'p_values_S_R'}, inplace=True)

df_amazon_sentimentR = pd.DataFrame(columns=['Cause', 'Effect'])
df_amazon_sentimentR["Cause"] = list(np.log(amazon_prices_2016_[1:]/amazon_prices_2016_[:-1]))
df_amazon_sentimentR["Effect"] = amazon_interpolated_2016_[1:]
granger_amazon_sentimentR = granger_causality_test(df_amazon_sentimentR, max_lag=15)
granger_amazon_sentimentR.rename(columns={'p_values': 'p_values_R_S'}, inplace=True)

# Differential sentiment and retweet-weighted sentiment
df_amazon_sentimentW = pd.DataFrame(columns=['Cause', 'Effect'])
df_amazon_sentimentW["Cause"] = amazon_interpolated_2016_W[1:]
df_amazon_sentimentW["Effect"] = np.log(amazon_prices_2016_[1:]/amazon_prices_2016_[:-1])
granger_amazon_sentimentW = granger_causality_test(df_amazon_sentimentW, max_lag=15)
granger_amazon_sentimentW.rename(columns={'p_values': 'p_values_S_R_RETweight'}, inplace=True)

df_amazon_sentimentRW = pd.DataFrame(columns=['Cause', 'Effect'])
df_amazon_sentimentRW["Cause"] = list(np.log(amazon_prices_2016_[1:]/amazon_prices_2016_[:-1]))
df_amazon_sentimentRW["Effect"] = amazon_interpolated_2016_W[1:]
granger_amazon_sentimentRW = granger_causality_test(df_amazon_sentimentRW, max_lag=15)
granger_amazon_sentimentRW.rename(columns={'p_values': 'p_values_R_S_RETweight'}, inplace=True)

# Combine all results into a single DataFrame
granger_output_amazon = granger_amazon_sentiment.merge(granger_amazon_sentimentR, on="Lag") \
                                                .merge(granger_amazon_sentimentW, on="Lag") \
                                                .merge(granger_amazon_sentimentRW, on="Lag")

granger_output_amazon

plot_lag_p_values3(granger_output_amazon,name="Amazon",columns= ['p_values_S_R','p_values_R_S','p_values_S_R_RETweight','p_values_R_S_RETweight'],text=["Sentiment Diff-->Returns", "Returns-->Sentiment Diff", "Weighted Sentiment Diff-->Returns", "Returns-->Weighted Sentiment Diff"])

"""# MICROSOFT SENTIMENT & WEIGHTED SENTIMENT DIFFERENTIAL"""

#MICROSOFT
# Set up the data for Microsoft
microsoft_interpolated = interpolate2(merged_sentiment_stockprice_microsoft)
microsoft_interpolated_2016_ = microsoft_interpolated[microsoft_interpolated.index >= "2016-01-01"].sentiment_difference.values

microsoft_interpolatedW = interpolate3(merged_weighted_sentiment_stockprice_microsoft)
microsoft_interpolated_2016_W = microsoft_interpolatedW[microsoft_interpolatedW.index >= "2016-01-01"].weighted_sentiment_difference.values

microsoft_prices_2016_ = microsoft_data_daily[microsoft_data_daily.index >= "2016-01-01"][["Adj Close"]].values

# Create DataFrames for sentiment and weighted sentiment
df_microsoft_sentiment = pd.DataFrame(columns=['Cause', 'Effect'])
df_microsoft_sentiment["Cause"] = microsoft_interpolated_2016_[1:]
df_microsoft_sentiment["Effect"] = np.log(microsoft_prices_2016_[1:]/microsoft_prices_2016_[:-1])
granger_microsoft_sentiment = granger_causality_test(df_microsoft_sentiment, max_lag=15)
granger_microsoft_sentiment.rename(columns={'p_values': 'p_values_S_R'}, inplace=True)

df_microsoft_sentimentR = pd.DataFrame(columns=['Cause', 'Effect'])
df_microsoft_sentimentR["Cause"] = list(np.log(microsoft_prices_2016_[1:]/microsoft_prices_2016_[:-1]))
df_microsoft_sentimentR["Effect"] = microsoft_interpolated_2016_[1:]
granger_microsoft_sentimentR = granger_causality_test(df_microsoft_sentimentR, max_lag=15)
granger_microsoft_sentimentR.rename(columns={'p_values': 'p_values_R_S'}, inplace=True)

# Differential sentiment and retweet-weighted sentiment
df_microsoft_sentimentW = pd.DataFrame(columns=['Cause', 'Effect'])
df_microsoft_sentimentW["Cause"] = microsoft_interpolated_2016_W[1:]
df_microsoft_sentimentW["Effect"] = np.log(microsoft_prices_2016_[1:]/microsoft_prices_2016_[:-1])
granger_microsoft_sentimentW = granger_causality_test(df_microsoft_sentimentW, max_lag=15)
granger_microsoft_sentimentW.rename(columns={'p_values': 'p_values_S_R_RETweight'}, inplace=True)

df_microsoft_sentimentRW = pd.DataFrame(columns=['Cause', 'Effect'])
df_microsoft_sentimentRW["Cause"] = list(np.log(microsoft_prices_2016_[1:]/microsoft_prices_2016_[:-1]))
df_microsoft_sentimentRW["Effect"] = microsoft_interpolated_2016_W[1:]
granger_microsoft_sentimentRW = granger_causality_test(df_microsoft_sentimentRW, max_lag=15)
granger_microsoft_sentimentRW.rename(columns={'p_values': 'p_values_R_S_RETweight'}, inplace=True)

# Combine all results into a single DataFrame
granger_output_microsoft = granger_microsoft_sentiment.merge(granger_microsoft_sentimentR, on="Lag") \
                                                       .merge(granger_microsoft_sentimentW, on="Lag") \
                                                       .merge(granger_microsoft_sentimentRW, on="Lag")

granger_output_microsoft

plot_lag_p_values3(granger_output_microsoft,name="Microsoft",columns= ['p_values_S_R','p_values_R_S','p_values_S_R_RETweight','p_values_R_S_RETweight'],text=["Sentiment Diff-->Returns", "Returns-->Sentiment Diff", "Weighted Sentiment Diff-->Returns", "Returns-->Weighted Sentiment Diff"])

"""# NETFLIX SENTIMENT & WEIGHTED SENTIMENT DIFFERENTIAL"""

# Set up the data for Netflix
netflix_interpolated = interpolate2(merged_sentiment_stockprice_netflix)
netflix_interpolated_2016_ = netflix_interpolated[netflix_interpolated.index >= "2016-01-01"].sentiment_difference.values

netflix_interpolatedW = interpolate3(merged_weighted_sentiment_stockprice_netflix)
netflix_interpolated_2016_W = netflix_interpolatedW[netflix_interpolatedW.index >= "2016-01-01"].weighted_sentiment_difference.values

netflix_prices_2016_ = netflix_data_daily[netflix_data_daily.index >= "2016-01-01"][["Adj Close"]].values

# Create DataFrames for sentiment and weighted sentiment
df_netflix_sentiment = pd.DataFrame(columns=['Cause', 'Effect'])
df_netflix_sentiment["Cause"] = netflix_interpolated_2016_[1:]
df_netflix_sentiment["Effect"] = np.log(netflix_prices_2016_[1:]/netflix_prices_2016_[:-1])
granger_netflix_sentiment = granger_causality_test(df_netflix_sentiment, max_lag=15)
granger_netflix_sentiment.rename(columns={'p_values': 'p_values_S_R'}, inplace=True)

df_netflix_sentimentR = pd.DataFrame(columns=['Cause', 'Effect'])
df_netflix_sentimentR["Cause"] = list(np.log(netflix_prices_2016_[1:]/netflix_prices_2016_[:-1]))
df_netflix_sentimentR["Effect"] = netflix_interpolated_2016_[1:]
granger_netflix_sentimentR = granger_causality_test(df_netflix_sentimentR, max_lag=15)
granger_netflix_sentimentR.rename(columns={'p_values': 'p_values_R_S'}, inplace=True)

# Differential sentiment and retweet-weighted sentiment
df_netflix_sentimentW = pd.DataFrame(columns=['Cause', 'Effect'])
df_netflix_sentimentW["Cause"] = netflix_interpolated_2016_W[1:]
df_netflix_sentimentW["Effect"] = np.log(netflix_prices_2016_[1:]/netflix_prices_2016_[:-1])
granger_netflix_sentimentW = granger_causality_test(df_netflix_sentimentW, max_lag=15)
granger_netflix_sentimentW.rename(columns={'p_values': 'p_values_S_R_RETweight'}, inplace=True)

df_netflix_sentimentRW = pd.DataFrame(columns=['Cause', 'Effect'])
df_netflix_sentimentRW["Cause"] = list(np.log(netflix_prices_2016_[1:]/netflix_prices_2016_[:-1]))
df_netflix_sentimentRW["Effect"] = netflix_interpolated_2016_W[1:]
granger_netflix_sentimentRW = granger_causality_test(df_netflix_sentimentRW, max_lag=15)
granger_netflix_sentimentRW.rename(columns={'p_values': 'p_values_R_S_RETweight'}, inplace=True)

# Combine all results into a single DataFrame
granger_output_netflix = granger_netflix_sentiment.merge(granger_netflix_sentimentR, on="Lag") \
                                                   .merge(granger_netflix_sentimentW, on="Lag") \
                                                   .merge(granger_netflix_sentimentRW, on="Lag")

granger_output_netflix

plot_lag_p_values3(granger_output_netflix,name="Netflix",columns= ['p_values_S_R','p_values_R_S','p_values_S_R_RETweight','p_values_R_S_RETweight'],text=["Sentiment Diff-->Returns", "Returns-->Sentiment Diff", "Weighted Sentiment Diff-->Returns", "Returns-->Weighted Sentiment Diff"])

"""# TESLA SENTIMENT & WEIGHTED SENTIMENT DIFFERENTIAL"""

#TESLA
# Set up the data for Tesla
tesla_interpolated = interpolate2(merged_sentiment_stockprice_tesla)
tesla_interpolated_2016_ = tesla_interpolated[tesla_interpolated.index >= "2016-01-01"].sentiment_difference.values

tesla_interpolatedW = interpolate3(merged_weighted_sentiment_stockprice_tesla)
tesla_interpolated_2016_W = tesla_interpolatedW[tesla_interpolatedW.index >= "2016-01-01"].weighted_sentiment_difference.values

tesla_prices_2016_ = tesla_data_daily[tesla_data_daily.index >= "2016-01-01"][["Adj Close"]].values

# Create DataFrames for sentiment and weighted sentiment
df_tesla_sentiment = pd.DataFrame(columns=['Cause', 'Effect'])
df_tesla_sentiment["Cause"] = tesla_interpolated_2016_[1:]
df_tesla_sentiment["Effect"] = np.log(tesla_prices_2016_[1:]/tesla_prices_2016_[:-1])
granger_tesla_sentiment = granger_causality_test(df_tesla_sentiment, max_lag=15)
granger_tesla_sentiment.rename(columns={'p_values': 'p_values_S_R'}, inplace=True)

df_tesla_sentimentR = pd.DataFrame(columns=['Cause', 'Effect'])
df_tesla_sentimentR["Cause"] = list(np.log(tesla_prices_2016_[1:]/tesla_prices_2016_[:-1]))
df_tesla_sentimentR["Effect"] = tesla_interpolated_2016_[1:]
granger_tesla_sentimentR = granger_causality_test(df_tesla_sentimentR, max_lag=15)
granger_tesla_sentimentR.rename(columns={'p_values': 'p_values_R_S'}, inplace=True)

# Differential sentiment and retweet-weighted sentiment
df_tesla_sentimentW = pd.DataFrame(columns=['Cause', 'Effect'])
df_tesla_sentimentW["Cause"] = tesla_interpolated_2016_W[1:]
df_tesla_sentimentW["Effect"] = np.log(tesla_prices_2016_[1:]/tesla_prices_2016_[:-1])
granger_tesla_sentimentW = granger_causality_test(df_tesla_sentimentW, max_lag=15)
granger_tesla_sentimentW.rename(columns={'p_values': 'p_values_S_R_RETweight'}, inplace=True)

df_tesla_sentimentRW = pd.DataFrame(columns=['Cause', 'Effect'])
df_tesla_sentimentRW["Cause"] = list(np.log(tesla_prices_2016_[1:]/tesla_prices_2016_[:-1]))
df_tesla_sentimentRW["Effect"] = tesla_interpolated_2016_W[1:]
granger_tesla_sentimentRW = granger_causality_test(df_tesla_sentimentRW, max_lag=15)
granger_tesla_sentimentRW.rename(columns={'p_values': 'p_values_R_S_RETweight'}, inplace=True)

# Combine all results into a single DataFrame
granger_output_tesla = granger_tesla_sentiment.merge(granger_tesla_sentimentR, on="Lag") \
                                              .merge(granger_tesla_sentimentW, on="Lag") \
                                              .merge(granger_tesla_sentimentRW, on="Lag")

granger_output_tesla

plot_lag_p_values3(granger_output_tesla,name="Tesla",columns= ['p_values_S_R','p_values_R_S','p_values_S_R_RETweight','p_values_R_S_RETweight'],text=["Sentiment Diff-->Returns", "Returns-->Sentiment Diff", "Weighted Sentiment Diff-->Returns", "Returns-->Weighted Sentiment Diff"])

